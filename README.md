# AI Task Automation Sandbox

This project simulates the kind of structured evaluation workflows. It handles AI-generated content, scores it for quality and optionally displays it in a Streamlit interface for manual review.

## Core Features

- LLM-based response generation
- Auto-scoring on:
  - Factual accuracy
  - Clarity and coherence
  - Tone
- CSV input/output
- Streamlit UI for interactive QA

## Project Structure

├── main.py # Core execution pipeline
├── evaluate.py # Evaluation logic (scoring)
├── streamlit_app.py # Streamlit UI
├── prompts.csv # Input prompts
├── responses.csv # Output file (autogenerated)
├── requirements.txt # Dependencies
└── README.md # This file

## How to Run

1. **Clone the repo**
   ```bash
   git clone https://github.com/Tostage/AI-task-automation-sandbox
   cd AI-task-automation-sandbox

2. **Install dependencies**
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt

3. **Run core logic**
   python main.py

4. **Optional: Launch UI**
   streamlit run streamlit_app.py
